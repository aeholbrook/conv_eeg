{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "net4nets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aeholbrook/net4nets/blob/main/net4nets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy27wB2buquF"
      },
      "source": [
        "I put all the dependency installs here, but I intend to go through and make sure we actually *need* all these at a later date. Obviously a git clone will make this easier too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzB4VUOgDf7x"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!git clone https://github.com/aeholbrook/net4nets /content/net4nets -q\n",
        "!pip install keras-applications -q\n",
        "!pip install mne -q\n",
        "!pip install tensorflow-gpu -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZMVg2okDkDX"
      },
      "source": [
        "print(\"importing requisite libraries...\")\n",
        "%cd /content/net4nets/\n",
        "from utils import *\n",
        "%cd /content/\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
        "mne.utils.set_config('MNE_USE_CUDA', 'true') \n",
        "%matplotlib inline\n",
        "!MNE_USE_CUDA=true python -c \"import mne; mne.cuda.init_cuda(verbose=True)\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8Tsb32ZZ9kU"
      },
      "source": [
        "#Data Reading and Cleanup\n",
        "\n",
        "##Read EEGLab files\n",
        "\n",
        "The first part of our notebook is primarily concerned with reading and cleaning the epoched EEG data processed and provided by the lab with which we are partnering. This data comes from the ***Integrative Neuroscience Laboratory at Southern Illinois University***. \n",
        "\n",
        "This section more formally defines how brain‐signal decoding can be viewed as a supervised classification problem and includes the notation used to describe the methods.\n",
        "\n",
        "We assume that we are given one EEG dataset per subject $i$. Each dataset is separated into labeled trials (time‐segments of the original recording that each belong to one of several classes). Concretely, we are given datasets $Di={(X_1,y_1),…,(X_{Ni},y_{Ni})}$, where $N_i$ denotes the total number of recorded trials for subject $i$. The input matrix $X_j \\in \\mathbb{R}^{E⋅T}$ of trial $j,1≤j≤N_i$ contains the preprocessed signals of $E$‐recorded electrodes and $T$‐discretized time steps recorded per trial.\n",
        "\n",
        "The corresponding class label of trial $j$ is denoted by $y\\ ^j$. It takes values from a set of $K$ class labels $L$ that, in our case, correspond to the experimental condition, for our binary classification example:\n",
        "\n",
        "$\\forall y_j:y_j∈L=\\{l_0=“Smoking”,l_1=“Abstaining”\\}$\n",
        "\n",
        "##Our Classifier\n",
        "\n",
        "The decoder $f$ is trained on these existing trials such that it is able to assign the correct label to new unseen trials. Concretely, we aim to train the decoder to assign the label $y_j$ to trial $X_j$ using the output of a parametric classifier $f(X_j;\\theta):\\mathbb{R}^{E⋅T}\\longrightarrow L$ with parameters $\\theta$.\n",
        "\n",
        "For the rest of this article, we assume that the classifier $f(X_j;\\theta)$ is represented by a standard machine‐learning pipeline decomposed into two parts: a first part that extracts a (vector‐valued) feature representation $\\varphi(X_j;\\theta\\varphi)$ with parameters $\\theta\\varphi$, which could either be set manually (for hand designed features), or learned from the data; and a second part consisting of a classifier g with parameters $\\theta g$ that is trained using these features, that is, $f(X_j;\\theta)=g(\\varphi(X_j; \\theta\\varphi),\\theta g)$.\n",
        "\n",
        "###File Notation\n",
        "\n",
        "Files were cleaned and processed as `.set` and `.fdt` format in a *MATLAB* plugin called *EEGLab*. The lab labeled the experimental conditions as follows, using an example file, `0011ec1LE_Bs.set`:\n",
        "\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "<td>001</td>\n",
        "<td>1</td>\n",
        "<td>ec1</td>\n",
        "<td>LE_Bs</td>\n",
        "<td>.set</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>Subject Number</td>\n",
        "<td>Experimental Group</td>\n",
        "<td>Experimental Condition</td>\n",
        "<td>Cleanup Info</td>\n",
        "<td>file type</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "###Epoch Array Organization\n",
        "\n",
        "We are most concerned here with the file type - we only want to try to read the `.fdt` files - and the experimental group - the 1 or 2 at the 4th position in the `filename` string. This will tell us if the data comes from the *first* condition - participants who had just smoked a cigarette - or the *second* condition - participants who had abstained from smoking for 24 hours before the session. \n",
        "\n",
        "Our code will iterate through a `list` of filenames in the `directory` of our choosing and, depending on whether `filename[3]` is a `1` or `2`, place read `mne.Epochs` objects into the array `cond_1` or `cond_2`, which will then be concatenated into one array `conditions` as an output.\n",
        "\n",
        "##Chunking Epochs\n",
        "\n",
        "In the interest of artifact reduction, we've decided that our model could be best trained and tested using **chunked** `mne.Epochs` objects. To accomodate this, the `read_files` function takes in a parameter `chunk_size` and merges individual `mne.Epochs` objects from each array into `mne.Epochs` objects, each containing a number of epochs equal to `chunk_size`. These chunks are read by the `process_data()` function later and averaged as though they were a single epoch.\n",
        "\n",
        "After they are **chunked**, these objects are concatenated into either the `cond_1` or `cond_2` array, which are eventually combined into `conditions` and returned.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_eLBoiPDoVm"
      },
      "source": [
        "def read_files(start=0, stop=10, chunk_size=2):\n",
        "  cond_1 = []\n",
        "  cond_2 = []\n",
        "\n",
        "  directory = \"/content/drive/MyDrive/Deep_EEG_Project\"\n",
        "  for filename in tqdm(os.listdir(directory)[start:stop], position=0, leave=True):\n",
        "      if filename.endswith(\".set\"): \n",
        "        if filename[3] == \"1\":\n",
        "          input = mne.io.read_epochs_eeglab(directory +\"/\"+ filename, verbose='CRITICAL')\n",
        "          input_list = [input[i:i + chunk_size] for i in range(0, len(input), chunk_size)]\n",
        "          cond_1.extend(input_list)\n",
        "        else:\n",
        "          input = mne.io.read_epochs_eeglab(directory +\"/\"+ filename, verbose='CRITICAL')\n",
        "          input_list = [input[i:i + chunk_size] for i in range(0, len(input), chunk_size)]\n",
        "          cond_2.extend(input_list)\n",
        "  conditions = [cond_1,cond_2]\n",
        "  return conditions"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQKrLZifwG2e"
      },
      "source": [
        "def process_data(conditions,condition_labels,f_low=4,f_high=30,f_bins=26, decim=1):\n",
        "  #This assigns an arbitrary number of condition labels with size corresponding to the number of items in their respective condition arrays.\n",
        "  labels_arr = np.concatenate([np.full(len(item),idx) for idx,item in enumerate(conditions)])\n",
        "  #A lambda function which performs a morlet power spectral density transformation on an inputted MNE Epochs object and outputs an array containing the spectral data\n",
        "  process_epochs = lambda epoch : np.asarray(mne.time_frequency.tfr_morlet(epoch, freqs = np.linspace(f_low, f_high, f_bins, endpoint=True), n_cycles=3, decim=decim, n_jobs=10, verbose = False)[0].data)\n",
        "  #this line maps the above lambda function to the conditions array and performs ops at C level, which is much faster than interpreter-level\n",
        "  conditions = np.concatenate([list(map(process_epochs, cond)) for cond in conditions])\n",
        "  return conditions, labels_arr, f_bins"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0yyzA08ZuWa"
      },
      "source": [
        "condition_labels = [\"Smoking\",\"Abstinence\"]"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jUNej48VFJW"
      },
      "source": [
        "###Reading the Files\n",
        "Now, we can read the input files into the session, and these two lines of code will aggregate them into a numpy array of the following shape:\n",
        "\n",
        "> `input_arr.shape` = $n_{epochs}$, $n_{electrodes}$, $n_{frequencies}$, $n_{measurements}$\n",
        "\n",
        "> `input_labels.shape` = $n_{epochs}$\n",
        "\n",
        "$n_{epochs}$ is the number of epochs in our sample divided by `chunk_size`,\n",
        "\n",
        "$n_{electrodes}$ is the number of electrodes provided in our data - here $29$.\n",
        "\n",
        "$n_{frequencies}$ = `f_bins`, or the number of discrete frequencies output by the morlet transformation.\n",
        "\n",
        "$n_{measurements}$ are the number of point samples performed per epoch, divided by `decim`.\n",
        "\n",
        "I've divided the data such that it can be processed and stored as a numpy array. I think this will be useful, especially for colab, which can crash quite often with the introduction of taxing data like the EEG data in this project. \n",
        "\n",
        "`read_and_save()` reads the files from their original EEGLab format and then saves them as a `numpy` array. It also specifies a lower and upper range in order to determine how many files you wish to obtain. The limit is in rthe ballpark of $500$. It's also a recursive function, which calls itself until it's reached the stop value and returns a full input array. It deletes the array every time it calls itself because otherwise it would take an unsustainable amount of RAM. The files don't take too long for numpy to read, anyway.\n",
        "\n",
        "In the interest of utilizing RAM conservatively (these files can get pretty big), I decided to load the arrays into local storage in the form of `.npy` `numpy` binary files. This is also useful because if RAM use gets too high and the runtime crashes, you don't have to reload all of the training and testing data. You can scale the file reading rate to your individual processing needs and ability.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "admH70hidUC-"
      },
      "source": [
        "def read_and_save(start=0, stop=400, increments = 100, input_arr = [], input_labels =[], filename=\"morlet_transform_\"):  \n",
        "  if (stop-start) % increments != 0:\n",
        "    raise Exception(\"Increments must divide files evenly!\")\n",
        "  if start <= stop-1:\n",
        "    print(\"\\nReading Files \", start, \" - \", start+increments-1, \"from directory...\")\n",
        "    conditions = read_files(start,start+increments-1)\n",
        "    print(\"\\nPerforming Morlet transformation on Epoch data\")\n",
        "    if (len(input_arr)==0):\n",
        "      input_arr, input_labels, f_bins = process_data(conditions, condition_labels) \n",
        "    else:\n",
        "      input_arr_temp, input_labels_temp, f_bins = process_data(conditions, condition_labels) \n",
        "      input_arr = np.concatenate((input_arr,input_arr_temp))\n",
        "      input_labels = np.concatenate((input_labels,input_labels_temp))\n",
        "      del input_arr_temp\n",
        "      del input_labels_temp\n",
        "    filename = \"morlet_transform_\"+str(start)+\"_\"+str(start+increments-1)\n",
        "    np.save(filename+\".npy\",input_arr)\n",
        "    np.save(filename+\"_labels.npy\",input_labels)\n",
        "    del conditions\n",
        "    del input_arr\n",
        "    del input_labels\n",
        "    return read_and_save(start+increments,stop,increments)\n",
        "  else:\n",
        "    return input_arr,input_labels\n",
        "  "
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrPzbf8tOXjn"
      },
      "source": [
        "def load_set(start=0,stop=400,increments=100,input_arr=[],input_labels=[],filename=\"morlet_transform_\"):\n",
        "  if (stop-start) % increments != 0:\n",
        "    raise Exception(\"Increments must divide files evenly!\")\n",
        "  if start <= stop-1:\n",
        "    filename_full = filename+str(start)+\"_\"+str(start+increments-1)\n",
        "    if len(input_arr)==0:\n",
        "      input_arr = np.load(filename_full+\".npy\")\n",
        "      input_labels = np.load(filename_full+\"_labels.npy\")\n",
        "    else:\n",
        "      input_arr = np.concatenate((input_arr,np.load(filename_full+\".npy\",mmap_mode=\"r\")))\n",
        "      print(\"done\")\n",
        "      input_labels = np.concatenate((input_labels,np.load(filename_full+\"_labels.npy\", mmap_mode=\"r\")))\n",
        "    return load_set(start+increments,stop,increments,input_arr,input_labels)\n",
        "  else:\n",
        "    return input_arr,input_labels"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieqFhJduB9br"
      },
      "source": [
        "#Read in the training array.\n",
        "input_arr, input_labels = read_and_save(0,400,400)\n",
        "#load the saved training array\n",
        "input_arr, input_labels = load_set(start=0,stop=400,increments=400)\n",
        "#Read in the testing array.\n",
        "input_arr_2, input_labels_2 = read_and_save(400,500,100)\n",
        "#load the saved testing array\n",
        "input_arr_2, input_labels_2 = load_set(start=400,stop=500,increments=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8Uke86lNI66"
      },
      "source": [
        "print(\"\\nShape of input array:\")\n",
        "print(input_arr.shape)\n",
        "print(\"\\nInput Labels Size:\")\n",
        "print(input_labels.shape)\n",
        "\n",
        "print(\"\\nShape of input array 2:\")\n",
        "print(input_arr_2.shape)\n",
        "print(\"\\nInput Labels 2 Size:\")\n",
        "print(input_labels_2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBGPhnzBfSZR"
      },
      "source": [
        "\n",
        "##EEG Data Visualization\n",
        "\n",
        "EEG data can sometimes be difficult to display, as it is 2-Dimensional data expressed over a period of time. However, since we performed the **Morlet Transform** on our EEG data, we now have an array expressing the **Power Spectral Density** at each site on our EEG cap.\n",
        "\n",
        "Because we averaged the data for several electrodes using `chunk_size`, the dataset we are currently working with is the averaged electrical activity measured at each of $29$ electrodes over a period of $4\\ *$ `chunk_size` seconds of measurement. This measurement is then **decimated**, such that the $2048$ point samples measured per $4$-second epoch are divided by the number represented by `decim`. Below are topographic and plotted representations of that data, for both the **smoking** and **nonsmoking** condition:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfuKDtYyJo7A"
      },
      "source": [
        "conditions[0][5].plot_psd_topomap()\n",
        "conditions[0][5].plot_psd(2,45)\n",
        "conditions[1][5].plot_psd_topomap()\n",
        "conditions[1][5].plot_psd(2,45)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2ZW07bd9Enj"
      },
      "source": [
        "##Input Value Shuffling\n",
        "\n",
        "This function will shuffle the input values in `input_arr`, which are currently sorted by condition, into a random assortment if conditions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWkzBHz19Dh1"
      },
      "source": [
        "def shuffle_and_split(input_arr, input_labels, split_val = 1, multiply_constant = 1000000000, axes_to_merge = (0,1), random_forest=False):\n",
        "  input_arr = np.concatenate(np.split(input_arr,split_val,axis=-1))\n",
        "  #input_arr *= multiply_constant\n",
        "  input_labels_split = np.repeat(input_labels,split_val)\n",
        "  indices = tf.range(start=0, limit=input_arr.shape[0], dtype=tf.int32)\n",
        "  shuffled_indices = tf.random.shuffle(indices)\n",
        "\n",
        "  input_arr = np.take(input_arr, shuffled_indices, axis=0)\n",
        "  shuffled_input_labels = np.take(input_labels_split, shuffled_indices, axis=0)\n",
        "\n",
        "  if random_forest:\n",
        "    input_arr = np.mean(input_arr, axis = -1)\n",
        "    input_arr = input_arr.reshape(input_arr.shape[0], -1)\n",
        "  else:\n",
        "    input_arr = np.mean(input_arr, axis = -1)\n",
        "    arr_shape = [input_arr.shape[item] for item in axes_to_merge]\n",
        "    arr_shape = np.append(arr_shape,-1)\n",
        "    input_arr = np.reshape(input_arr,arr_shape)\n",
        "    print(input_arr.shape)\n",
        "\n",
        "  return input_arr, shuffled_input_labels, split_val"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eOVLHZ4Zj2X"
      },
      "source": [
        "##Random Forest\n",
        "\n",
        "Here, we are building and training a ***Random Forest Classifier***, the performance of which will be compared to that of the ***Convolutional Neural Network*** we will eventually train to make predictions on this data.\n",
        "\n",
        "[Here is a fun video outlining the construction and interpretation of a ***Random Forest*** machine learning model.](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ&vl=pt)\n",
        "\n",
        "###Inputs to Model\n",
        "\n",
        "This random forest will use the `shuffle_and_split()` function to shuffle the `input_arr` and `input_labels` arrays in order to better randomize the data which will be presented to the model. \n",
        "\n",
        "Then, we will use the `train_test_split()` function to separate the shuffled data and labels into `train` and `test` sections, with the `test_size` parameter indicating the ratio of `test` data to `train` data. \n",
        "\n",
        "It's important to split the data into test and train sets, in order to avoid overfitting your model on the data it's seeing.\n",
        "\n",
        "###Outputs to Model\n",
        "\n",
        "We are using the `sklearn` `metrics` library to estimate the accuracy of the ***Random Forest*** model on data it has not yet seen. The command `metrics.accuracy_score(y_test, y_pred_2)` compares the model's predictions for the test set, and the actual values for this set.\n",
        "\n",
        "Additionally, we can use our classifier's `feature_importances_` attribute to read out a list of how much value the classifier places on each feature in our dataset, although in the data given that may not be extremely informative as to what is driving differences between conditions in our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNYi60uIw5-r"
      },
      "source": [
        "shuffled_input_array, shuffled_input_labels, split_val = shuffle_and_split(input_arr, input_labels, axes_to_merge=(0,1,2,-1), random_forest=True)\n",
        "x_train,x_test,y_train,y_test = train_test_split(shuffled_input_array,shuffled_input_labels,test_size=0.2)\n",
        "del shuffled_input_array, shuffled_input_labels\n",
        "shuffled_input_array_2, shuffled_input_labels_2, split_val = shuffle_and_split(input_arr_2, input_labels_2, axes_to_merge=(0,1,2,-1), random_forest=True)\n",
        "x_train_2,x_test_2,y_train_2,y_test_2 = train_test_split(shuffled_input_array_2,shuffled_input_labels_2,test_size=0.2)\n",
        "del shuffled_input_array_2, shuffled_input_labels_2"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDXFIZOEw3tK"
      },
      "source": [
        "clf2 = RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
        "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
        "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "            min_samples_leaf=1, min_samples_split=2,\n",
        "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=100,\n",
        "            oob_score=False, random_state=None, verbose=1,\n",
        "            warm_start=False)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpdTX801Exiq"
      },
      "source": [
        "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "clf2.fit(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VhPbyDLebZB"
      },
      "source": [
        "y_pred=clf2.predict(x_test)\n",
        "y_pred_2=clf2.predict(x_test_2)\n",
        "\n",
        "feature_imp_2 = pd.Series(clf2.feature_importances_).sort_values(ascending=False)\n",
        "print(\"Accuracy on Test Set 1:\",metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"Accuracy on Test Set 2:\",metrics.accuracy_score(y_test_2, y_pred_2))\n",
        "\n",
        "print(\"Mean Feature Importance:\",np.mean(feature_imp_2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J_AA0J-6NaP"
      },
      "source": [
        "del clf2"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A89OnefLH053"
      },
      "source": [
        "##Test and Train Split\n",
        "\n",
        "Similar to above, we will be shuffling our data using the `shuffle_and_split()` function, then using the `train_test_split()` function to separate the shuffled data and labels into `train` and `test` sections, with the `test_size` parameter indicating the ratio of test data to train data. \n",
        "\n",
        "We need to use the `tf.expand.dims` command to add an extra axis to the test and train inputs. `tensorflow` will use this extra dimension to store the ***features*** its **convolution layers** will be trained to detect. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgEp6VJeED0z"
      },
      "source": [
        "shuffled_input_arr, shuffled_input_labels, split_val = shuffle_and_split(input_arr, input_labels, axes_to_merge=(0,1), random_forest=False)\n",
        "x_train,x_test,y_train,y_test = train_test_split(shuffled_input_arr,shuffled_input_labels,test_size=0.2)\n",
        "x_train = tf.expand_dims(x_train, axis=-1)\n",
        "x_test = tf.expand_dims(x_test, axis=-1)\n",
        "print(\"X Train:\", x_train.shape)\n",
        "print(\"Y Train:\", y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfl3p4D2x0LP"
      },
      "source": [
        "shuffled_input_arr_2, input_labels_2, split_val = shuffle_and_split(input_arr_2, input_labels_2, axes_to_merge=(0,1), random_forest=False)\n",
        "x_train_2,x_test_2,y_train_2,y_test_2 = train_test_split(shuffled_input_arr_2,input_labels_2,test_size=0.2)\n",
        "x_train_2 = tf.expand_dims(x_train_2, axis=-1)\n",
        "x_test_2 = tf.expand_dims(x_test_2, axis=-1)\n",
        "print(\"X Train:\", x_train_2.shape)\n",
        "print(\"Y Train:\", y_train_2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-1RzqNrqS9w"
      },
      "source": [
        "we evaluated Rectified Linear Units (ReLUs, $f(x)=max(x,0)$)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYIY3O8aJpLH"
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  input_shape = x_train.shape[1:]\n",
        "  #input_shape = (4656, 29, 26, 1)[1:]\n",
        "  model.add(keras.Input(shape=input_shape))\n",
        "\n",
        "  model.add(keras.layers.Conv2D(26,(3,3),activation='relu', input_shape = input_shape))\n",
        "  model.add(keras.layers.Conv2D(26,(3,3),activation='relu'))\n",
        "  model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "  model.add(keras.layers.Dropout(.2))\n",
        "  model.add(keras.layers.Dense(3000, activation='relu'))\n",
        "  model.add(keras.layers.Flatten())\n",
        "  model.add(keras.layers.Dense(1,activation='sigmoid'))\n",
        "  model.build()\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fjUSFLS_M__",
        "outputId": "41785e07-abd5-4344-ff31-37885e640c21"
      },
      "source": [
        "model = build_model()\n",
        "\n",
        "#tf.keras.utils.plot_model(model, rankdir=\"TB\", show_shapes=True, show_layer_names=False)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 27, 24, 26)        260       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 25, 22, 26)        6110      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 12, 11, 26)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 12, 11, 26)        0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 12, 11, 3000)      81000     \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 396000)            0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 396001    \n",
            "=================================================================\n",
            "Total params: 483,371\n",
            "Trainable params: 483,371\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfhW-0bEJRAn"
      },
      "source": [
        "##Baseline Model\n",
        "\n",
        "First, we are going to evaluate the efficacy of our model with randomized weights, but the same basic structure as the final model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FP___mE7JRzd",
        "outputId": "efa3d6b6-c92a-4ad0-8f6d-354ddf3569e8"
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "baseline_values = model.test_on_batch(x_test[:],y_test[:])\n",
        "print(\"Baseline Loss:\",baseline_values[0])\n",
        "print(\"Baseline Accuracy:\",baseline_values[1])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline Loss: 0.6931471824645996\n",
            "Baseline Accuracy: 0.4974226951599121\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-p8Y9LiOpop"
      },
      "source": [
        "##Model Training\n",
        "\n",
        "asdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afimSVuwPG-9"
      },
      "source": [
        "batch_size = 1\n",
        "epochs = 60\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "saving_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"model_data/curr_model\",\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks=[saving_callback,early_stop_callback])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvNiU0O7pVsZ"
      },
      "source": [
        "test_set_1_values = model.test_on_batch(x_test[:], y_test[:])\n",
        "\n",
        "\n",
        "print(\"Test Set 1 Loss:\",test_set_1_values[0])\n",
        "print(\"Test Set 1 Accuracy:\",test_set_1_values[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGfXN16rPI2B"
      },
      "source": [
        "test_set_2_values = model.test_on_batch(x_train_2[:300], y_train_2[:300])\n",
        "\n",
        "\n",
        "print(\"Test Set 2 Loss:\",test_set_2_values[0])\n",
        "print(\"Test Set 2 Accuracy:\",test_set_2_values[1])\n",
        "\n",
        "#model.save(\"curr_model\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEawHnXjVyZq"
      },
      "source": [
        "Here is where I am going to add some test data from the other side of the fileset so the net is getting data it's never seen before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwsf8XvwB0GV"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4KL2Is1N3J7"
      },
      "source": [
        "start = 0\n",
        "stop = 100\n",
        "print(\"Reading Files \", start, \" - \", stop, \"from directory...\")\n",
        "conditions = read_files(start,stop)\n",
        "print(\"Performing Morlet transformation on Epoch data\")\n",
        "input_arr, input_labels, f_bins = process_data(conditions, condition_labels)\n",
        "np.save(\"morlet_transform\", start, \"-\", stop, \".npy\",input_arr)\n",
        "np.save(\"morlet_transform\", start, \"-\", stop, \"_labels.npy\",input_arr,input_labels)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
